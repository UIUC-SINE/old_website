<!DOCTYPE html>
<html lang=en>

<head>
  <title>Introduction</title>
  <meta name="viewport" content="width=device-width">
  <link rel="stylesheet" type="text/css" href="/styles.css" /> 
  <link rel="stylesheet" type="text/css" href="/codehilite.css" /> 
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/mathtex-script-type.min.js" defer></script> 



</head>
<body>
<main>
  <article>
    <header>
      <h1>Introduction</h1>
      <time datetime="2019-06-16">2019-06-16</time>
    </header>
    <div class="toc">
<ul>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#a-comment-on-notation">A Comment on Notation</a></li>
</ul>
</li>
<li><a href="#area-based-registration">Area based Registration</a><ul>
<li><a href="#correlation-like-methods">Correlation-like Methods</a><ul>
<li><a href="#cross-correlation">Cross Correlation</a></li>
<li><a href="#generalized-iterative-cross-correlation">Generalized Iterative Cross Correlation</a></li>
<li><a href="#selective-similary-detection-algorithm-ssda">Selective Similary Detection Algorithm (SSDA)</a></li>
</ul>
</li>
<li><a href="#frequency-based-methods">Frequency Based Methods</a><ul>
<li><a href="#phase-correlation">Phase Correlation</a></li>
<li><a href="#de-castro-morandi-method">De Castro, Morandi Method</a></li>
<li><a href="#phase-correlation-rotationscale-extension">Phase Correlation - Rotation/Scale Extension</a></li>
<li><a href="#phase-correlation-subpixel-registration-extension">Phase Correlation - Subpixel Registration Extension</a></li>
</ul>
</li>
<li><a href="#information-based-methods">Information Based Methods</a><ul>
<li><a href="#mutual-information">Mutual Information</a></li>
</ul>
</li>
<li><a href="#optimization-based-methods">Optimization Based Methods</a><ul>
<li><a href="#nmsre-conjugate-descent">NMSRE Conjugate Descent</a></li>
<li><a href="#gauss-newton-minimization">Gauss-Newton Minimization</a></li>
<li><a href="#levenberg-marquardt">Levenberg-Marquardt</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#feature-based-registration">Feature Based Registration</a><ul>
<li><a href="#harris-corner-detection-and-ransac">Harris Corner Detection and RANSAC</a></li>
</ul>
</li>
<li><a href="#summary">Summary</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>
<h1 id="introduction">Introduction</h1>
<p><em>Motion estimation</em>, a related field, is the process of identifying motion captured in a series of images (usually frames of a video).  This motion may be due to motion of the camera which causes the whole scene to appear to move (<em>apparent motion</em>), or individual objects moving independently within the frame.  In motion fields, a velocity vector is associated with each pixel in a particular region of the image (<em>local</em> motion estimation) or the image as a whole (<em>global</em> motion estimation).  These motion vectors usually represent 2D motion across the image, but are sometimes 3D to capture movement in 3D space.  When a motion field for individual pixels has been computed it is common to group motion vectors that belong to the same moving object, a process known as <em>motion segmentation</em>. <sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup></p>
<p><em>Image registration</em> is the process of overlaying two images of the same scene taken at different times, different viewpoints, or with different imaging equipment.  Registration is an important step in countless fields in remote sensing.  Geographic information systems (GIS), medical computer tomography (CT), cartography and computer vision all make extensive use of image registration for the purposes of image fusion and denoising, change detection or super-resolution.</p>
<p>Let <script type="math/tex">i_1</script> and <script type="math/tex">i_2</script> be two images captured of a scene.  These are often called the <em>reference</em> and <em>template</em> images.  In image registration, we want to find a mapping from regions in the template image to regions in the reference image.  More formally, we want to find <script type="math/tex">f</script> such that</p>
<p>
<script type="math/tex; mode=display">
i_2(\bm{x}) = g(i_1(f(\bm{x})), \bm{x}) \,\, \forall \bm{x} \in X
</script>
</p>
<p>where <script type="math/tex">\bm{x}</script> is a 2D coordinate in the image overlap region <script type="math/tex">X</script>, <script type="math/tex">f</script> is some unknown coordinate transform, and <script type="math/tex">g</script> is an unknown intensity mapping function.  <script type="math/tex">g</script> is often assumed to be unitary, but can be a very complicated function in multimodal applications like medical imaging where <script type="math/tex">i_1</script> and <script type="math/tex">i_2</script> are captured from different instruments.</p>
<p><em>Zitova, Flusser 2003</em> generalizes all image registration algorithms into 4 steps. <sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>  Note that certain algorithms omit some of these steps.</p>
<ol>
<li><strong>Feature detection</strong> - Distinct features (points, edges, closed regions, intersections, corners, etc.) are detected in both images.  These features may be represented by coordinates (intersections, corners, etc.), coordinate pairs (edges) or a more complex parameterization.  This step is omitted in area based registration methods.</li>
<li><strong>Feature Matching</strong> - Correspondence is established between features detected in the images.  Feature similarity measures or feature positions within the images may be used to do this.  This step is omitted in area based registration methods.</li>
<li><strong>Transform model estimation</strong> - The parameters of the coordinate mapping function <script type="math/tex">f</script> are computed using the previously matched features.</li>
<li><strong>Image transformation</strong> - The sensed image is transformed using the estimated parameters and optionally fused with the template image.  Interpolation may be necessary if the mapping function contains non-integer coordinates.</li>
</ol>
<p>In the next section, I highlight some classical and/or popular contemporary image registration methods and the domains in which they are applied.</p>
<h3 id="a-comment-on-notation">A Comment on Notation</h3>
<p>This document contains many types of variables which can represent transform parameters, <!-- placeholder variables inside optimizations, --> 1D vectors of parameters and 2D images.   I try to follow these guidelines for easier reading:</p>
<ul>
<li>bold for variables which represent 1D vectors.  For example <script type="math/tex">\bm{x}</script> is a coordinate vector representing position within an image</li>
<li>superscript <script type="math/tex">*</script> for ground truth parameters of <script type="math/tex">f</script>.  For example <script type="math/tex">s^*</script> and <script type="math/tex">\theta^*</script> are parameters controlling scaling and rotation</li>
<li>hat <script type="math/tex">\,\hat{}\,</script> for algorithmic estimates of ground truth parameters. For example <script type="math/tex">\hat{\theta}</script> may represent the best estimate for <script type="math/tex">\theta^*</script> found by an algorithm
<!-- * hat <script type="math/tex">\,\hat{}\,</script> for placeholder variables in maximization or minimization problems. For example <script type="math/tex">\hat{\theta}</script> may represent the current value under test in an iterative algorithm searching for <script type="math/tex">\theta_0</script> -->
<!-- * superscript <script type="math/tex">*</script> for final parameter estimates obtained by registration methods. For example <script type="math/tex">\theta^*</script> is a best estimate for the true <script type="math/tex">\theta_0</script> --></li>
</ul>
<h1 id="area-based-registration">Area based Registration</h1>
<p>There are two overarching categories of image registration algorithms, area based and feature based.  Area based methods perform no feature detection or feature mapping and instead use all available pixels in the feature matching step <!-- not necessarily true --> to register the images.  Most classical registration methods fall into this category and they are relatively simpler than feature based methods as a result.</p>
<p>This simplicity can sometimes come at a performance cost in certain situations, however.  If a target scene has smooth regions with few salient features to register on, an area based method may perform more poorly than a feature based method which can ignore the smooth parts of the scene that contain little alignment information.</p>
<h3 id="correlation-like-methods">Correlation-like Methods</h3>
<h5 id="cross-correlation">Cross Correlation</h5>
<p>The most straightforward of all methods, direct correlation, is conceptually simple and works for many classes of transformations.</p>
<p>Note that the normalization here is crucial so that the intensities of <script type="math/tex">i_1</script> and <script type="math/tex">i_2</script> do not influence the maximum.  We call this measure normalized cross correlation (NCC). <sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup></p>
<!-- only works when $f$ is a simple linear translation, $f(\bm{x}) = \bm{x} - \bm{c}^*$. -->

<p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{f} &= \arg \max_{f \in F} \text{NCC}(i_1, i_2(f)) \\
&= \arg \max_{f \in F} \frac{\sum_{\bm{x} \in X} i_1(\bm{x})i_2(f(\bm{x}))}{\sqrt{\sum_{\bm{x} \in X} i_1(\bm{x})^2}}
\end{aligned}
</script>
</p>
<p>where <script type="math/tex">F</script> is the set of all linear translations.  Practically, the time required to search the space of all possible transformations for a given application makes this approach infeasible, so <script type="math/tex">F</script> is often restricted to translations.</p>
<p>
<script type="math/tex; mode=display">
\hat{c} = \arg \max_{\bm{c}} \frac{\sum_{\bm{x} \in X} i_1(\bm{x})i_2(\bm{x} - \bm{c})}{\sqrt{\sum_{\bm{x} \in X} i_1(\bm{x})^2}}
</script>
</p>
<!-- $$ f^* = \arg \max_{\hat{f}} \sum_x i_1(x)i_2(\hat{f}(x)) = \arg \min_{\hat{f}} \sum_x e(x)^2 $$ -->

<p>Related similarity measures which are sometimes used in place of normalized cross correlation are sum of squared error (SSE)</p>
<p>
<script type="math/tex; mode=display">
\text{SSE}(i_1, i_2(f)) = \sum_{\bm{x} \in X} (i_1(\bm{x}) - i_2(f(\bm{x})))^2
</script>
</p>
<p>and correlation coefficient</p>
<p>
<script type="math/tex; mode=display">
\text{Corr}(i_1, i_2(f)) = \frac{\text{cov}(i_1, i_2(f))}{\sigma_1 \sigma_2} = \frac{\sum_{\bm{x} \in X} (i_1(\bm{x}) - \mu_1)(i_2(f(\bm{x})) - \mu_2)}{\sqrt{\sum_{\bm{x} \in X} (i_1(\bm{x}) - \mu_1)^2 \sum_{\bm{x} \in X}(i_2(f(\bm{x})) - \mu_2)^2}}
</script>
</p>
<p>where <script type="math/tex">\mu_1</script>, <script type="math/tex">\mu_2</script>, <script type="math/tex">\sigma_1</script>, <script type="math/tex">\sigma_2</script> are the means and variances of <script type="math/tex">i_1</script> and <script type="math/tex">i_2</script> over <script type="math/tex">X</script>.</p>
<p>While cross correlation methods are very old, they continue to see widespread use because of ease of implementation in hardware (NCC can be efficiently implemented using multiply-accumulate hardware) and because limiting <script type="math/tex">f</script> to translations isn't significantly restrictive for many scenarios.</p>
<p>In practice though, cross correlation is still successful in the presence of slight rotation, scaling or even non-affine transformation. <!-- need to find citation --></p>
<h5 id="generalized-iterative-cross-correlation">Generalized Iterative Cross Correlation</h5>
<h5 id="selective-similary-detection-algorithm-ssda">Selective Similary Detection Algorithm (SSDA)</h5>
<p>In standard correlation methods, the sum over <script type="math/tex">X</script> for each candidate <script type="math/tex">f</script> must be computed in full before a maximum is found.  Barnea, Silverman <sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup> propose a class alternative schemes which greatly improve computation time in two ways.  The paper calls these algorithms selective similarity detection algorithms (SSDAs), of which one is presented here.</p>
<p>First, the paper uses absolute sum of errors (ASE) as a similarity measure, which requires no costly multiplications unlike NCC or SSE.</p>
<p>
<script type="math/tex; mode=display">
ASE(i_1, i_2(f)) = \sum_{\bm{x} \in X} |i_1(\bm{x}) - i_2(\bm{x} - \bm{c})|
</script>
</p>
<p>The second optimization uses early stopping and requires that the sum over <script type="math/tex">X</script> be implemented sequentially (e.g. as an iterative software loop).  For a particular candidate <script type="math/tex">\bm{c}</script>, the current value of the sum is compared to a threshold parameter after each iteration.  If the ASE surpasses this threshold, the number of iterations is recorded and the algorithm moves on to the next candidate.  If an candidate computation never exceeds <script type="math/tex">T</script>, then the final ASE is recorded instead.</p>
<p>Finally, the candidate with the lowest ASE is selected.  If all candidates surpassed the threshold, then the candidate with the most number of iterations before passing the threshold is selected.</p>
<p>
<figure><img src="ssda.png" /><figcaption>Error accumulation curves for candidates <script type="math/tex">\bm{c}_1</script>, <script type="math/tex">\bm{c}_2</script>, <script type="math/tex">\bm{c}_3</script> and <script type="math/tex">\bm{c}_4</script>.  Error computation for <script type="math/tex">\bm{c}_1</script> and <script type="math/tex">\bm{c}_2</script> terminated early at 7 and 9 iterations.  <script type="math/tex">\bm{c}_4</script> is the best estimate for offset, followed by <script type="math/tex">\bm{c}_3</script>, <script type="math/tex">\bm{c}_2</script> and <script type="math/tex">\bm{c}_1</script>.</figcaption>
</figure>
</p>
<p>This algorithm offers potentially orders of magnitude speed improvements over direct cross-correlation because of early stopping, but requires selection of parameter <script type="math/tex">T</script>.  A choice of <script type="math/tex">T</script> too high limits efficiency gains while a choice of <script type="math/tex">T</script> too low can lead to suboptimal results.</p>
<h3 id="frequency-based-methods">Frequency Based Methods</h3>
<p>If an acceleration over correlation based methods is needed or the images were acquired under frequency dependent noise, Fourier methods are often preferred.  These methods exploit the Fourier representation of images in the frequency domain and have shown better robustness against illumination differences between <script type="math/tex">i_1</script> and <script type="math/tex">i_2</script>.</p>
<h5 id="phase-correlation">Phase Correlation</h5>
<p>Phase correlation was originally proposed for registering linearly translated images.  It takes advantage of the Fourier Shift theorem, which states that translating an image and taking its Fourier transform is equivalent to multiplying the Fourier transform of the original untranslated image by a complex exponential.</p>
<p>Computing the cross-power spectral density (CPSD) we can directly obtain this complex exponential.</p>
<p>
<script type="math/tex; mode=display">
i_2(\bm{x}) = i_1(\bm{x} - \bm{c}^*)
</script>
</p>
<p>
<script type="math/tex; mode=display">
CPSD(i_1, i_2)(\bm{\omega}) = \frac{I_1(\bm{\omega}) \overline{I_2(\bm{\omega})}}{|I_1(\bm{\omega}) \overline{I_2(\bm{\omega})}|} =
\frac{I_1(\bm{\omega}) \overline{I_1(\bm{\omega}) e^{-j \langle \bm{\omega}, \bm{c}^* \rangle}}}{|I_1(\bm{\omega}) \overline{I_1(\bm{\omega}) e^{-j \langle \bm{\omega}, \bm{c}^* \rangle}}|} = e^{j \langle \bm{\omega},  \bm{c}^* \rangle}
</script>
</p>
<p>Where <script type="math/tex">I_1</script> and <script type="math/tex">I_2</script> are the Fourier transforms of <script type="math/tex">i_1</script> and <script type="math/tex">i_2</script>.  The final estimate for <script type="math/tex">\bm{c}_0</script> is obtained by a final inverse Fourier transform of the CPSD, yielding a delta at location <script type="math/tex">\bm{c}_0</script>.</p>
<p>
<script type="math/tex; mode=display">
PC(i_1, i_2)(\bm{x}) = \mathcal{F}^{-1}\left[ CPSD(i_1, i_2) \right](\bm{x}) = \delta(\bm{x} - \bm{c}^*) \\
\hat{c} = \arg \max_{\bm{x}} PC(i_1, i_2)(\bm{x})
</script>
</p>
<p>An important consideration here is that the Fourier Shift theorem only holds exactly when translation is circular.  In practice, the phase correlation method still works if the region of overlap is sufficiently large.  Foroosh, Zerubia, Berthod <sup id="fnref:7"><a class="footnote-ref" href="#fn:7">7</a></sup> propose a prefilter which can be applied to both images before phase correlation to reduce these effects.</p>
<h5 id="de-castro-morandi-method">De Castro, Morandi Method</h5>
<p>De Castro, Morandi <sup id="fnref:5"><a class="footnote-ref" href="#fn:5">5</a></sup> introduced an extension of the phase correlation method which applies to images that are translated and rotated. This method is similar in spirit to the Generalized Iterative Cross Correlation method in that it amounts to repeatedly detransforming <script type="math/tex">i_2</script> with different parameters, testing alignment with <script type="math/tex">i_1</script> using some similarity measure and repeating this process until the correct parameters are found.  Like the standard phase correlation method, this technique claims robustness against frequency dependent noise and non-uniform illumination differences between the images.</p>
<p>Let <script type="math/tex">i_2</script> be a translated and rotated copy of <script type="math/tex">i_1</script>.  Then</p>
<p>
<script type="math/tex; mode=display">
i_2(\bm{x}) = i_1(R_{\theta^*} (\bm{x} - \bm{c^*}))
</script>
</p>
<p>where </p>
<p>
<script type="math/tex; mode=display">
R_{\theta^*} = \begin{bmatrix} \cos \theta^* & - \sin \theta^* \\ \sin \theta^* & \cos \theta^* \end{bmatrix}
</script>
</p>
<p>is a rotation operator of angle <script type="math/tex">\theta^*</script>.</p>
<p>From the Fourier shift theorem, we know that a shift by <script type="math/tex">\bm{c}^*</script> in the spatial domain results in a multiplication by a complex exponential in the frequency domain.  Additionally, the Fourier rotation theorem tells us that a rotation in the spatial domain is a rotation by the same angle in the frequency domain.  Therefore, the relation between <script type="math/tex">I_1</script> and <script type="math/tex">I_2</script> can be written</p>
<p>
<script type="math/tex; mode=display">
I_2(\bm{\omega}) = e^{-j \langle \bm{\omega}, \bm{c}^* \rangle} I_1(R_{\theta^*} \bm{\omega})
</script>
</p>
<!-- $$ -->

<!-- CPSD(I_1, \hat{I_2})(\omega) = \frac{I_1(\omega) \overline{I_2(R^{-1}_{\hat{\theta}}\omega)}}{|I_1(\omega) \overline{I_2(R^{-1}_{\hat{\theta}}\omega)}|} = -->

<!-- \frac{I_1(\omega) \overline{I_1(R^{-1}_{\hat{\theta}} R_{\theta} \omega)}}{|I_1(\omega) \overline{I_1(R^{-1}_{\hat{\theta}} R_{\theta} \omega)}|} = -->

<!-- \frac{I_1(\omega) \overline{I_1(R_{\theta - \hat{\theta}} \omega)}}{|I_1(\omega) \overline{I_1(R_{\theta - \hat{\theta}} \omega)}|} = -->

<!-- $$ -->

<p>To find <script type="math/tex">\theta^*</script>, the authors consider the expression</p>
<p>
<script type="math/tex; mode=display">
\mathcal{F}^{-1} \left[ \frac{I_2(\bm{\omega})}{I_1(R_{\theta} \bm{\omega})} \right]
</script>
</p>
<p>When <script type="math/tex">\theta = \theta^*</script>, we get</p>
<p>
<script type="math/tex; mode=display">
\mathcal{F}^{-1} \left[ \frac{I_2(\bm{\omega})}{I_1(R_{\theta} \bm{\omega})} \right] =
\mathcal{F}^{-1} \left[ \frac{e^{-j \langle \bm{\omega}, \bm{c}^* \rangle}I_1(R_{\theta^*}\bm{\omega})}{I_1(R_{\theta^*} \bm{\omega})} \right]  = \mathcal{F}^{-1} \left[ e^{-j \langle \bm{\omega}, \bm{c}^* \rangle} \right] = \delta(\bm{x} - \bm{c}^*)
</script>
</p>
<p>By testing a range of values for <script type="math/tex">\theta</script> for which results in the closest to an impulse in the above expression, an approximate for the true <script type="math/tex">\theta^*</script> can be found.</p>
<p>Formally, this is the minimization problem</p>
<p>
<script type="math/tex; mode=display">
\hat{\theta} = \arg \min_{\theta} \left\Vert \mathcal{F}^{-1} \left[ 
\frac{I_2(\bm{\omega})}{I_1(R_{\theta}\bm{\omega})}
\right] \right\Vert_N
</script>
</p>
<p>where <script type="math/tex">\left\Vert \right\Vert_N</script> is a placeholder for a measure which is large for unit impulses.  For example</p>
<p>
<script type="math/tex; mode=display">
\left\Vert f \right\Vert_N = \left\Vert f - \delta(\bm{x} - \arg \max_{\hat{\bm{x}}} f(\hat{\bm{x}})) \right\Vert_2
</script>
</p>
<p>When <script type="math/tex">\hat{\theta}</script> has been found, the offset can be obtained directly from the impulse function</p>
<p>
<script type="math/tex; mode=display">
\hat{\bm{c}} = \arg \max_\bm{x} \mathcal{F}^{-1} \left[ \frac{I_2(\bm{\omega})}{I_1(R_{\hat{\theta}} \bm{\omega})} \right](\bm{x})
</script>
</p>
<p>Note that the denominator <script type="math/tex">I_1(R_{\hat{\theta}} \bm{\omega})</script> must be evaluated using interpolation, as <script type="math/tex">R_{\hat{\theta}} \bm{\omega}</script> will not coincide with the sample nodes of <script type="math/tex">I_1</script> in general.</p>
<!-- FIXME author somehow use window to eliminate edge effects? -->

<!-- FIXME need a notational correction here, there are continuous and discrete versions of I in both time and frequency that need to be expressed -->

<!-- \tilde{I}(\omega_x, \omega_y) = \int_X I(x, y) e^{-j(x \omega_x + y \omega_y)} dx\, dy \approx \Delta x \Delta y \tilde{I}(\omega_x, \omega_y) -->

<h5 id="phase-correlation-rotationscale-extension">Phase Correlation - Rotation/Scale Extension</h5>
<p>Phase correlation in its original form is an elegant method of registering translated images.  The method introduced by De Castro and Morandi provides a way to detect and correct for rotation in images before applying phase correlation.  However, another common transformation in imaging systems is scaling, which can occur when the target scene moves closer to the imaging device or if the imager has zoom capabilities.  Like rotation, scaling by a factor <script type="math/tex">s_0</script> can also be written as a matrix operator like so</p>
<p>
<script type="math/tex; mode=display">
S_{s^*} = \begin{bmatrix}s^* & 0 \\ 0 & s^*\end{bmatrix}
</script>
</p>
<p>Sarvaiya, Patnaik, Kothari <sup id="fnref:8"><a class="footnote-ref" href="#fn:8">8</a></sup> introduced a new method which is capable of registering images that have been translated, rotated and scaled.  They make use of the Fourier scale, Fourier shift and Fourier rotation properties and also the Log-Polar transform (also known as Fourier-Mellin transform), where rotation and scaling in the original domain manifest as translation in the Log-Polar domain.  Their approach is broken into two applications of the phase correlation method, where the first application is used to recover scale and rotation, and the second, translation.</p>
<p>If <script type="math/tex">i_2</script> is a scaled, rotated and shifted copy of <script type="math/tex">i_1</script>,</p>
<p>
<script type="math/tex; mode=display">
i_2(\bm{x}) = i_1(R_{\theta^*} S_{s^*} \bm{x} - \bm{c}_0)
</script>
</p>
<p>
<script type="math/tex; mode=display">
\begin{aligned}
&PC \left( \left| \mathcal{LP} \left[ \mathcal{F} \left[ i_1 \right] \right] \right| , \left| \mathcal{LP} \left[ \mathcal{F} \left[ i_2 \right]\right] \right| \right)(x, y) \\
= &PC \left( \left| \mathcal{LP} \left[ \mathcal{F} \left[ i_1 \right] \right] \right| , \left| \mathcal{LP} \left[ \mathcal{F} \left[ i_1(R_{\theta^*} S_{s^*} \bm{x} - \bm{c}_0) \right] \right] \right| \right)(x, y)  \\
&\text{Apply Fourier shift, scale, rotation properties} \\
= &PC \left( \left| \mathcal{LP} \left[ I_1(\bm{\omega}) \right] \right| , \left| \mathcal{LP} \left[ \frac{1}{s^{*2}} e^{-j \langle S_{s^*}^{-1} R_{\theta^*} \bm{\omega}, \bm{c}_0 \rangle} I_1(S_{s^*}^{-1} R_{\theta^*} \bm{\omega}) \right] \right| \right)(x, y) \\
= &PC \left( \left| \mathcal{LP} \left[ I_1(\bm{\omega}) \right] \right| , \left| \mathcal{LP} \left[ \frac{1}{s^{*2}} e^{-j \langle S_{s^*}^{-1} R_{\theta^*} \bm{\omega}, \bm{c}_0 \rangle} \right] \mathcal{LP} \left[ I_1(S_{s^*}^{-1} R_{\theta^*} \bm{\omega}) \right] \right| \right)(x, y) \\
&\text{Apply Log-Polar shift property} \\
= &PC \left( \left| \mathcal{LP} \left[ I_1 \right](\rho, \theta) \right| , \left| \mathcal{LP} \left[ \frac{1}{s^{*2}} e^{-j \langle S_{s^*}^{-1} R_{\theta^*} \bm{\omega}, \bm{c}_0 \rangle} \right] \mathcal{LP} \left[ I_1 \right](\rho + \ln \frac{1}{s^*}, \theta + \theta_0) \right| \right)(x, y) \\
= &PC \left( \left| \mathcal{LP} \left[ I_1 \right](\rho, \theta) \right| , \left| \mathcal{LP} \left[ I_1 \right](\rho + \ln \frac{1}{s^*}, \theta + \theta_0) \right| \right)(x, y) \\
= &\delta \left(x - \ln \frac{1}{s^*}, y - \theta_0 \right)
\end{aligned}
</script>
</p>
<p>where <script type="math/tex">\mathcal{LP}</script> is the Log-Polar transform.</p>
<h5 id="phase-correlation-subpixel-registration-extension">Phase Correlation - Subpixel Registration Extension</h5>
<p>Todo: import from previous report.</p>
<h3 id="information-based-methods">Information Based Methods</h3>
<p>Viola and Wells introduced a new class of registration methods in 1994 based on entropy of image pairs.  This class of methods has proven effective in multimodal registration so it has achieved significant popularity in medical imaging.</p>
<h5 id="mutual-information">Mutual Information</h5>
<p>In the early 20th century, Hartley was looking for a way to measure the transmission of information, particularly in relation to the telegraph as a communications system.  He considered a system in which a finite set of symbols ('dit' or 'dah', telegraph 1's or 0's) are sent sequentially through a channel (telegraph wire).</p>
<p>The number of unique messages that can be encoded given a message length of <script type="math/tex">n</script> and <script type="math/tex">s</script> unique symbols is <script type="math/tex">s^n</script>.  However, Hartley wanted a measure that would grow linearly with message length.  A message which is twice as long should contain twice as much information.</p>
<p>He therefore settled on the following measure of information:</p>
<p>
<script type="math/tex; mode=display">
H = n \log s = \log s^n
</script>
</p>
<p>From the first formulation, it is apparent that information grows linearly with <script type="math/tex">n</script>.  Another interesting feature of this measure is that if there is only one symbol, we know exactly what the message will be and so it contains no information <script type="math/tex">(H = n \log 1 = 0)</script>.  This suggests that entropy can also be viewed as a measure of uncertainty.</p>
<p>A disadvantage of Hartley's entropy measure is that it assumes all symbols are equally likely to occur in a message, which is generally not true.</p>
<p>In 1948, Shannon introduced a new measure of information which takes this fact into account by weighting Hartley's entropy by the probability that symbols occur.  This is now known as Shannon entropy.  For a set of <script type="math/tex">s</script> symbols with probabilities <script type="math/tex">p_1, ..., p_s</script> of occuring, Shannon entropy is given as</p>
<p>
<script type="math/tex; mode=display">
H = \sum_k p_k \log \frac{1}{p_k} = - \sum_i p_k \log p_k
</script>
</p>
<p>Like Hartley's entropy, Shannon entropy can be viewed as a measure of uncertainty.  If a particular symbol has a very high probability of occuring, our uncertainty about the message decreases and hence information decreases.  If all symbols have an equal probability of occuring, entropy is maximized.  Thus Shannon entropy may also be considered as a measure of spread of a probability distribution.  A distribution with most mass concentrated around a few peaks will have low entropy while a more uniform distribution will have higher entropy.</p>
<p>To compute Shannon entropy of an image, all possible intensity values of the pixels can be interpreted as symbols in the message.  For an image with bit depth of 8 bits, one can collect all the intensity values into a histogram in order to compute <script type="math/tex">p_0, ..., p_{255}</script>, as shown below.</p>
<p>
<figure><img src="histogram.png" /><figcaption>An image and its intensity histogram</figcaption>
</figure>
</p>
<p>Now that we can compute entropy for images we must introduce one more concept before registration can occur, joint histograms.  A joint histogram is a 2D function which, for all possible pairs of intensities, describes how many times intensity pairs occur for a pair of registered images.  For example, if a joint histogram has value 17 at coordinate [33, 34], then for this particular registration there are 17 pixels in which the first image has intensity 33 and the second has intensity 34.  In the case of two 8 bit images, the joint histogram is a 256x256 image.  An example joint histogram for two images is shown below.</p>
<p>
<figure><img src="feature_space.png" /><figcaption>Two registered images and their joint histogram, or feature space</figcaption>
</figure>
</p>
<p>The joint histogram changes with the alignment of the images.  For a correctly aligned pair of images, structures within the image align and vary with each other, so we expect the intensities to correlate which manifests as clustering in the joint histogram.  As the image pair becomes misaligned, more greyscale combinations are introduced and the joint histogram exhibits more uniformity.  By measuring this uniformity we now have a similarity measure for registration.</p>
<p>Formally, the joint Shannon entropy for a pair of registered images</p>
<p>
<script type="math/tex; mode=display">
H(i_1, i_2(f)) = -\sum_{m, n} p(m, n) \log p(m, n)
</script>
</p>
<p>where <script type="math/tex">p(i, j)</script> is the joint histogram of <script type="math/tex">i_1</script> and candidate registered <script type="math/tex">i_2(f)</script> in the region of overlap <script type="math/tex">X</script>. <!-- FIXME --></p>
<p>However, a problem that can occur when joint entropy is used directly is that low entropy (high degree of reported alignment) can occur for invalid registrations if the images contain large regions of uniform intensity.  For example, if the images in the figure above are aligned so that only their corners containing background overlap, the joint histogram will have approximately a single peak and the joint entropy will be very low.  To account for this, one can make use of the marginal entropies to penalize alignments where the region <script type="math/tex">X</script> contains little information in the images.  This is known as mutual information.</p>
<p>
<script type="math/tex; mode=display">
MI(i_1, i_2(f)) = H(i_1) + H(i_2(f)) - H(i_1, i_2(f))
</script>
</p>
<p>With this new measure, if the overlap region contains little information, terms <script type="math/tex">H(i_1)</script> and <script type="math/tex">H(i_2(f))</script> will be small and counteract joint entropy.  Also note that since mutual information contains <script type="math/tex">-H(i_1, i_2(f))</script>, minimizing joint entropy is related to maximizing mutual information.</p>
<h3 id="optimization-based-methods">Optimization Based Methods</h3>
<p>Many of the above methods introduce various similarity metrics as measures of alignment between registration candidates.  They mostly rely on an exhaustive search through all potential registration candidates and as such are restricted to situations where the set of all possible <script type="math/tex">f</script> candidates are small, usually simple translation.  In situations where the number of parameters controlling <script type="math/tex">f</script> is large, such as elastic transformation, it is appropriate to make use optimization methods that can find minima or maxima in the chosen similarity measure in a feasible amount of time.</p>
<h5 id="nmsre-conjugate-descent">NMSRE Conjugate Descent</h5>
<!-- FIXME assumes linear shift, but this section is about high dimensional f -->

<p>The first method, presented by Guizar-Sicairos, Thurman, Fienup <sup id="fnref:6"><a class="footnote-ref" href="#fn:6">6</a></sup>,  is a form of conjugate descent on the normalized root mean squared error (NRMSE), which is a translation-invariant measure of error between an image <script type="math/tex">f</script> and a copy <script type="math/tex">g</script> shifted by <script type="math/tex">(x_0^{\ast}, y_0^*)</script>.</p>
<p>
<script type="math/tex; mode=display">
f^*_{NMSRE} = \min_{\hat{f}} \frac{\sum_{x} |i_2(\hat{f}(x)) - i_1(x)|^2}{\sum_{x}|i_1(x)|^2}
</script>
</p>
<p>By minimizing the NMSRE over <script type="math/tex">f</script>, the true parameters of <script type="math/tex">f</script> can be found.</p>
<p>Rewriting the above definition into a maximization problem and assuming <script type="math/tex">f</script> is a linear shift by <script type="math/tex">[x_0, y_0]</script>, we can achieve a more useful formulation.</p>
<p>
<script type="math/tex; mode=display">
NMSRE^2 = 1 - \frac{\max_{x_0, y_0} |r(x_0, y_0)|^2}{\sum_{x, y}|i_1(x, y)|^2 \sum_{x, y}|i_2(x, y)|^2} \\
</script>
</p>
<p>
<script type="math/tex; mode=display">
\begin{aligned}
r(x_0, y_0) &= \sum_{x, y}I_2(x - x_0, y - y_0)I_1^*(x, y) \\
&= \sum_{u, v} \tilde{I_1}(u, v)\tilde{I_2}^*(u, v) \text{exp}\left[ j 2 \pi \left( u \frac{x_0}{M} + v \frac{y_0}{N} \right) \right]
\end{aligned}
</script>
</p>
<p>where <script type="math/tex">r</script> is cross correlation and <script type="math/tex">\tilde{I_1}</script> and <script type="math/tex">\tilde{I_2}</script> are the image DFTs of size <script type="math/tex">M \times N</script>.</p>
<p>Since all other terms are constant, we need only minimize <script type="math/tex">|r(x_0, y_0)|^2</script>.</p>
<p>
<script type="math/tex; mode=display">
\frac{d(r(x_0, y_0))}{dx_0} = 2 \text{Im} \left(r(x_0, y_0) \sum_{u, v} \frac{2 \pi u}{M} \tilde{I_1}^*(u, v) \times \tilde{I_2}(u, v) \text{exp} \left[-j 2 \pi \left( u \frac{x_0}{M} + v \frac{y_0}{N} \right) \right] \right) </script>
</p>
<p>With this partial derivative (and a similar for <script type="math/tex">y_0</script>) we can use standard conjugate descent to solve for <script type="math/tex">(x_0^{\ast}, y_0^*)</script>.</p>
<h5 id="gauss-newton-minimization">Gauss-Newton Minimization</h5>
<h5 id="levenberg-marquardt">Levenberg-Marquardt</h5>
<h1 id="feature-based-registration">Feature Based Registration</h1>
<p>Feature based methods involve a preprocessing step known as <em>feature detection</em>, where notable structures in both images are located and recorded.  Detected structures come in a variety of forms, such as polygons (forests, lakes, fields), line segments (roads, buildings), or single points (street intersections, region corners).  There are many algorithms capable of extracting these features and the optimal choice of algorithm is highly dependent on the target scene.  In general, a desirable property of these algorithms is that the same features can be detected in both images and that these features are robust against corruption introduced by function <script type="math/tex">g</script> or noise. <sup id="fnref2:2"><a class="footnote-ref" href="#fn:2">2</a></sup></p>
<p>In the next stage, <em>feature matching</em>, detected features are corresponded from both images.</p>
<h5 id="harris-corner-detection-and-ransac">Harris Corner Detection and RANSAC</h5>
<h1 id="summary">Summary</h1>
<p>Area based methods are preferred when images have salient details and information is provided by pixel intensities rather than shapes and structures in the imae.  The images' intensities must be similar or at least statistically related.  The set of candidate transformations <script type="math/tex">f</script> that can be searched is generally limited to translation with small amounts of rotation or skew, but there are some extensions to methods that support large rotations or skews so long as the degrees of freedom of <script type="math/tex">f</script> remains small.  Pyramid search techniques and sophisticated optimization strategies are available to more quickly find extrema of the similarity measure.</p>
<p>Feature based methods are generally utilized when shapes in structures in the image pair contain more alignment information than the pixel intensities, such as between a picture of an object and a computer model of an object.  The drawback of these methods is that such features can be hard to detect and match with each other.  It is critical that chosen feature detector be robust against any differences between the images.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Application</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>foo</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="references">References</h1>
<ul>
<li><a href="https://www.google.com/search?q=survey%20of%20mutual%20information%20based%20registration">A Survey of Mutual Information Based Registration</a> - Pluim, Maintz, Viergever 2003</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Motion Estimation - Konrad&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p><a href="https://www.sciencedirect.com/science/article/pii/S0262885603001379/pdfft?md5=9ac6884a88ac624d4861de8fe7666e27&amp;pid=1-s2.0-S0262885603001379-main.pdf">Image registration methods: a survey</a> - Zitova, Flusser 2003&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>Brown Survey Paper&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5008923">Barnea, Silverman 1972</a>&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:5">
<p><a href="https://ieeexplore.ieee.org/document/4767966">De Castro, Morandi 1987</a>&#160;<a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:6">
<p><a href="https://www.osapublishing.org/ol/viewmedia.cfm?uri=ol-33-2-156&amp;seq=0">Guizar-Sicairos, Thurman, Fienup 2008</a>&#160;<a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:7">
<p><a href="https://ieeexplore.ieee.org/document/988953">Foroosh, Zerubia, Berthod 2002</a>&#160;<a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:8">
<p><a href="http://www.jprr.org/index.php/jprr/article/view/355">Sarvaiya, Patnaik, Kothari 2012</a>&#160;<a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn:9">
<p>Feature-Based Deformable Image Registration with RANSAC Based Search Correspondence - Colleu, Shen, Matuszewski, Shark, Cariou&#160;<a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="fn:10">
<p>An Automatic Satellite Image Registration Technique Based on Harris Corner Detection and Random Sample Consensus (RANSAC) Outlier Rejection Model - Misra, Moorthi, Dhar, Ramakrishnan&#160;<a class="footnote-backref" href="#fnref:10" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
</ol>
</div>
  </article>
</main>
<footer>
    <p>SINE UIUC</p>
</footer>
</body>
</html>