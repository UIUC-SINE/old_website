<!DOCTYPE html>
<html lang=en>

<head>
    <title>Thesis</title>
    <meta name="viewport" content="width=device-width">
    <link rel="stylesheet" type="text/css" href="/styles.css" /> 
    <link rel="stylesheet" type="text/css" href="/codehilite.css" /> 
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/mathtex-script-type.min.js" defer></script> 

</head>
<body>
<main>
  <article>
    <header>
      <h1>Thesis</h1>
      <time datetime="2019-06-16">2019-06-16</time>
    </header>
    <style>
c { 
    color: gray;
} 
n {
    color: green;
}
header h1 {
    border: none;
}
h1 {
    /* border-bottom: 1px solid black; */
    border: 1px solid black;
    padding: 1em;
    text-align: center;
}
h2 {
    /* text-decoration: underline; */
    border-bottom: 1px solid black;
    font-size: 1.25em;
}
h3 {
    font-size: 1em;
}
</style>

<div class="toc">
<ul>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#registration-problem-model">Registration Problem Model</a></li>
<li><a href="#categorizing-registration-methods">Categorizing Registration Methods</a></li>
<li><a href="#a-comment-on-notation">A Comment on Notation</a></li>
<li><a href="#types-of-image-transforms">Types of Image Transforms</a><ul>
<li><a href="#translation">Translation</a></li>
<li><a href="#rotation">Rotation</a></li>
<li><a href="#scaling">Scaling</a></li>
<li><a href="#affine">Affine</a></li>
<li><a href="#perspective">Perspective</a></li>
<li><a href="#elastic">Elastic</a></li>
</ul>
</li>
<li><a href="#visors-mission">VISORS Mission</a></li>
<li><a href="#paper-outline">Paper Outline</a></li>
</ul>
</li>
<li><a href="#review-of-registration-methods">Review of Registration Methods</a></li>
<li><a href="#subpixel-registration">Subpixel Registration</a></li>
<li><a href="#multiframe-subpixel-registration">Multiframe Subpixel Registration</a></li>
<li><a href="#numerical-experiments">Numerical Experiments</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>
<h1 id="introduction">Introduction</h1>
<!--
- fields where imaging is used
  - remote sensing
    - change detection
    - image mosaicing
    - super resolution
  - medicine
    - combining CT and MRI data
    - overlaying patient data on anatomical references

-->

<p><em>Image registration</em> is the process of transforming multiple snapshots so that subjects or features common to two or more snapshots are aligned.
The images may be stitched into a composite image to get a wider field of view, higher resolution, reduced noise, or may be simply be aligned as in the case of video stabilization.  Depending on the problem, registration algorithms often need to contend with changes in the scene being imaged (due to elapsed time between snapshots), perspective changes (changes in camera position), and illumination changes (from different imaging equipment). <sup id="fnref:brown"><a class="footnote-ref" href="#fn:brown">3</a></sup></p>
<p><em>Motion estimation</em>, a related field, is the process of identifying motion captured in a series of images (usually frames of a video).  This motion may be due to motion of the camera which causes the whole scene to appear to move (<em>apparent motion</em>), or individual objects moving independently within the frame.  In motion fields, a velocity vector is associated with each pixel in a particular region of the image (<em>local</em> motion estimation) or the image as a whole (<em>global</em> motion estimation).  These motion vectors usually represent 2D motion across the image, but may also be 3D to capture movement in 3D space.  When a motion field for individual pixels has been computed it is common to group motion vectors that belong to the same moving object, a process known as <em>motion segmentation</em>. <sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup></p>
<p>Registration is an important step in the image processing pipeline for countless fields.  For example, in remote sensing applications registration is used in change detection, image mosaicing, and super resolution.  In medical imaging applications, registration is used for overlaying patient images from multiple channels, such as CT and MRI, which the caregiver can cross-reference for diagnoses, and to compare patient data to physiological atlases.</p>
<p>In the next sections, I mathematically describe the problem of image registration, provide a categorization framework for registration methods, and explain the motivating problem of this thesis.</p>
<h2 id="registration-problem-model">Registration Problem Model</h2>
<p>Let <script type="math/tex">i_1</script> and <script type="math/tex">i_2</script> be two images captured of a scene.  These are often called the <em>reference</em> and <em>sensed</em> images.  In image registration, we want to find a mapping from regions in the sensed image to regions in the reference image.  More formally, we want to find <script type="math/tex">f</script> such that</p>
<p>
<script type="math/tex; mode=display">
i_2(\bm{x}) = g(i_1(f(\bm{x})), \bm{x}) \,\, \forall \bm{x} \in X
</script>
</p>
<p>where <script type="math/tex">\bm{x}</script> is a coordinate vector in the image overlap region <script type="math/tex">X</script>, <script type="math/tex">f</script> is some unknown coordinate transform, and <script type="math/tex">g</script> is an unknown intensity mapping function.  <script type="math/tex">g</script> is often assumed to be unitary, but can be a very complicated function in multimodal applications like medical imaging where <script type="math/tex">i_1</script> and <script type="math/tex">i_2</script> are captured from different instruments.  For the rest of this paper, I assume <script type="math/tex">i_1</script> and <script type="math/tex">i_2</script> are 2D vectors containing image data.</p>
<!-- X is intersection region for SR, fusion -->

<!-- X is union for stitching -->

<h2 id="categorizing-registration-methods">Categorizing Registration Methods</h2>
<p>While there is a wide variety of approaches to the problem of image registration, many algorithms can be broken down into 4 steps which aids in their classification. <sup id="fnref:zitova"><a class="footnote-ref" href="#fn:zitova">2</a></sup>.</p>
<ol>
<li><strong>Feature detection</strong> - Distinct features (points, edges, closed regions, intersections, corners, etc.) are detected in both images.  These features may be represented by coordinates (intersections, corners, etc.), coordinate pairs (edges) or a more complex parameterization.  This step is omitted in non-feature based registration methods.</li>
<li><strong>Feature Matching</strong> - Correspondence is established between features detected in the images.  Feature similarity measures or feature positions within the images may be used to do this.  This step is omitted in non-feature-based registration methods.</li>
<li><strong>Transform model estimation</strong> - In feature-based methods, the parameters of the coordinate mapping function <script type="math/tex">f</script> are computed using the previously matched features.  In non-feature-based methods, model parameters can be estimated from image statistics, iterative cost minimization, or image spectra, to name a few.  This step is where most variability between registration methods lies.</li>
<li><strong>Image transformation</strong> - The sensed image is transformed using the estimated parameters and optionally fused with the reference image.  Interpolation may be necessary if the mapping function contains non-integer coordinates.</li>
</ol>
<p>The registration algorithms reviewed later in this manuscript perform a single pass of these steps to arrive at the registered result, but some other algorithms, especially those used in the process of super-resolution, repeat steps through several iterations and only stop when some criterion is met. <sup id="fnref:farsiu2004"><a class="footnote-ref" href="#fn:farsiu2004">5</a></sup></p>
<!-- In the next section, I highlight some classical and/or popular contemporary image registration methods and the domains in which they are applied. -->

<h2 id="a-comment-on-notation">A Comment on Notation</h2>
<p>This document contains many types of variables which can represent transform parameters, <!-- placeholder variables inside optimizations, --> 1D vectors of parameters and 2D images.   I try to follow these guidelines for easier reading:</p>
<ul>
<li>bold for variables which represent 1D vectors.  For example <script type="math/tex">\bm{x}</script> is a coordinate vector representing position within an image</li>
<li>superscript <script type="math/tex">*</script> for ground truth parameters of coordinate transform <script type="math/tex">f</script>.  For example <script type="math/tex">s^*</script> and <script type="math/tex">\theta^*</script> are parameters controlling scaling and rotation</li>
<li>hat <script type="math/tex">\,\hat{}\,</script> for algorithmic estimates of ground truth parameters. For example <script type="math/tex">\hat{\theta}</script> represents the estimates for <script type="math/tex">\theta^*</script> found by a particular algorithm
<!-- * hat <script type="math/tex">\,\hat{}\,</script> for placeholder variables in maximization or minimization problems. For example <script type="math/tex">\hat{\theta}</script> may represent the current value under test in an iterative algorithm searching for <script type="math/tex">\theta_0</script> -->
<!-- * superscript <script type="math/tex">*</script> for final parameter estimates obtained by registration methods. For example <script type="math/tex">\theta^*</script> is a best estimate for the true <script type="math/tex">\theta_0</script> --></li>
</ul>
<h2 id="types-of-image-transforms">Types of Image Transforms</h2>
<p>The coordinate transform is a fundamental component of any registration algorithm.  Most registration algorithms describe a specific class of coordinate transforms which can be completely described by a handful of parameters that are searched over during the <em>transform model estimation</em> step .  In this section, I describe a few of the most common classes of coordinate transforms, their parameters, and give some examples of where they are used.</p>
<h3 id="translation">Translation</h3>
<p>The simplest and most common type of coordinate transform is translation</p>
<p>
<script type="math/tex; mode=display">f(\bm{x}) = \bm{x} - \bm{c}</script>
</p>
<!-- FIXME citation -->

<p>where <script type="math/tex">\bm{c}</script> is a length 2 vector whose elements correspond to the shift in each dimension.  Some of the oldest registration methods operate over this class of transforms. </p>
<h3 id="rotation">Rotation</h3>
<p>Another type of registration method is rotation, in which the sensed image is rotated about some point.</p>
<p>
<script type="math/tex; mode=display">f(\bm{x}) = R_{\theta}\bm{x}</script>
</p>
<p>where <script type="math/tex">R_{\theta}</script> is known as a <em>rotation matrix</em>.  <script type="math/tex">R_{\theta}</script> has orthogonal columns and can be entirely parameterized by <script type="math/tex">\theta</script>, the rotation angle.</p>
<p>
<script type="math/tex; mode=display">
R_{\theta} = \begin{bmatrix} \cos \theta & - \sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}
</script>
</p>
<p>These two transform classes might be used together when stitching images from a digital microscope to get a larger field of view where the specimen slide is allowed to translate or rotate in a fixed plane.</p>
<h3 id="scaling">Scaling</h3>
<p>A third type of coordinate transform is scaling, where the sensed image origin and orientation remain fixed, but coordinates are scaled.</p>
<p>
<script type="math/tex; mode=display">f(\bm{x}) = S_s \bm{x}</script>
</p>
<p>where <script type="math/tex">S_s</script> is a scaling matrix parameterized by the scaling factor <script type="math/tex">s</script>.</p>
<p>
<script type="math/tex; mode=display">S_s = \begin{bmatrix} s & 0 \\ 0 & s \end{bmatrix}</script>
</p>
<p>These three coordinate transforms taken together are often called an <em>similarity transform</em>.  Similarity transforms are rigid, meaning they do not change the shape of features in the reference image, parallel lines remain parallel, and angles and lengths are preserved.  For example, a triangle in the sensed image will map to a similar triangle in the reference image.</p>
<p>Similarity transforms can be written generally as</p>
<p>
<script type="math/tex; mode=display">f(\bm{x}) = R_{\theta} S_s \bm{x} - \bm{c}</script>
</p>
<p>Some authors allow for the first or second column of <script type="math/tex">R_{\theta}</script> to be negated which corresponds to a geometric reflection, though this is less useful in registration settings.</p>
<h3 id="affine">Affine</h3>
<p>A generalization of the similarity transform is the <em>affine transform</em>, where the rotation matrix is replaced by a matrix with no orthogonality constraint and the scaling factor is incorporated.</p>
<p>
<script type="math/tex; mode=display">f(\bm{x}) = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \bm{x} - \bm{c}</script>
</p>
<p>This transform can also account geometric skew, where angles and lengths are no longer preserved but parallel lines remain parallel.</p>
<h3 id="perspective">Perspective</h3>
<p><n>
Another coordinate transform is the <em>perspective transform</em>, which occurs when a 3D scene is viewed through an idealized optical system and projected onto a 2D plane.  Perspective distortion causes objects which are further away from the camera lens to appear smaller in a process known as <em>foreshortening</em> <sup id="fnref2:brown"><a class="footnote-ref" href="#fn:brown">3</a></sup>.  If the coordinates of a visible point in 3D space is known, for example <script type="math/tex">[x',\, y',\, z']</script>, then its location within the image can be computed as
</n></p>
<p>
<script type="math/tex; mode=display">\bm{x} = [x,\, y]^T = \left[ \frac{-fx'}{z' - f},\, \frac{-fy'}{z' - f} \right]^T</script>
</p>
<p><n>
where <script type="math/tex">f</script> is the distance of the camera lens from the image plane.
</n></p>
<p><img alt="" src="perspective.png" style="width:500px" /></p>
<p><n>
In the context of image registration, we would like to know how changes in the relative position and orientation between the camera and observed scene affect projected coordinates.  However, it is not possible to express the relationship between <script type="math/tex">i_1</script> and <script type="math/tex">i_2</script> as <script type="math/tex">i_2(\bm{x}) = i_1(f(\bm{x}))</script>.</p>
<p><n>
In many remote sensing applications, the distance to the scene is much larger than the distance to the lens center (<script type="math/tex">z' \gg f</script>) and the spacecraft's field of view is narrow.  In this setting, it is possible to show that small changes in attitude manifest as apparent translational motion.
</n></p>
<p><c>
WIP Proof
</c></p>
<p><n>
With this result, the coordinate transform has now been simplified to
</n></p>
<p>
<script type="math/tex; mode=display">f(\bm{x}) = R_{\theta} \left(\bm{x} - [f \tan(\alpha),\, f \tan(\phi)]^T \right)</script>
</p>
<p><n>
where <script type="math/tex">\theta</script>, <script type="math/tex">\phi</script> and <script type="math/tex">\alpha</script> are the spacecraft roll, pitch and azimuth.
</n></p>
<p><img alt="" src="perspective_approx.png" style="width:600px" /></p>
<h3 id="elastic">Elastic</h3>
<p>When the type of transform is unknown or more complicated, elastic transforms can be used to correct for misalignment.  This can arise in situations where a 3D scene is projected through an optical system onto a 2D plane, but unlike perspective projection, there are large depth variations which occlude parts of the scene.  Objects which appear in one image may be completely obscured in the other, which is increasingly difficult to account for as the number of occlusions increases.  In these settings, a more general coordinate transform which can map more arbitrary distortions is preferred.</p>
<p>Elastic methods can either operate over the image as a whole (global), or apply different transforms to regions of the image separately (local).</p>
<p>An example of a global elastic method is the <em>bivariate polynomial</em> transform, in which the x and y coordinates are fed through a pair of polynomial functions.</p>
<p>
<script type="math/tex; mode=display">u = \sum_{l=0}^m \sum_{j=0}^i a_{ij} x^l y^{j - l}</script>
</p>
<p>
<script type="math/tex; mode=display">v = \sum_{l=0}^m \sum_{j=0}^i b_{ij} x^l y^{j - l}</script>
</p>
<p>where <script type="math/tex">[x,\,y]</script> are the original coordinates, <script type="math/tex">[u,\,v]</script> the transformed coordinates, and <script type="math/tex">a_{ij}</script> and <script type="math/tex">b_{ij}</script> are constant parameters controlling the transform.</p>
<p>Local methods are more general than global methods and can handle distortions that global methods cannot, such as deformable objects, complex 3D surfaces, and object motion within the scene.  While these methods are more powerful, there is a tradeoff with computational complexity as the number of parameters increases.  An example of a local elastic coordinate transform is piecewise spline interpolation.</p>
<p>Below is a diagram which illustrates examples of some of these transform classes.</p>
<p><img alt="" src="transforms.png" style="width:300px" /></p>
<!-- | Transform   | Properties                                | Applications | -->

<!-- |-------------|-------------------------------------------|--------------| -->

<!-- | Similarity  | Preserves angles, lengths, parallel lines |              | -->

<!-- | Affine      | Preserves parallel lines                  |              | -->

<!-- | Perspective |                                           |              | -->

<!-- | Projective  |                                           |              | -->

<!-- | Elastic     |                                           |              | -->

<h2 id="visors-mission">VISORS Mission</h2>
<p>The VIrtual Super-resolution Optics with Reconfigurable Swarms (VISORS) mission, due to be launched by NASA in 2023, is a heliophysics CubeSat mission designed to study the Sun's corona at a finer scale than has been achieved in previous missions in order to shine light on the processes which drive heating in the corona.</p>
<p>VISORS consists of two 3U spacecraft known as the Optics Spacecraft (OSC) and Detector Spacecraft (DSC), which carry instrumentation for taking measurements in extreme ultraviolet (EUV) range.  These two spacecraft will fly in formation 40 meters apart aligned along an axis pointed at the region of interest on the Sun during science mode.  The OSC focuses incoming light using a novel diffractive element known as a photon sieve while simultaneously using its solar panels to block off-axis light from entering the DSC.  The DSC will be positioned on the focal plane corresponding to He II emission line at 30.4nm.  In particular, the OSC uses a diffractive optical element known as a photon sieve, which can outperform equivalent reflective optics due to tighter manufacturing tolerances. <sup id="fnref:oktem"><a class="footnote-ref" href="#fn:oktem">4</a></sup></p>
<p>VISORS is what is known as a <em>virtual</em> telescope.  In contrast to other non-virtual space telescopes such as Hubble (visible light) and the Solar Dynamic Observatory (EUV), the focusing optics and detector fly on separate spacecraft which allows the design to support large focal lengths without significantly increasing spacecraft volume and to reconfigure the wavelength after launch by adjusting spacecraft separation.</p>
<p>In addition to its contributions to heliophysics, VISORS will serve as a technology demonstration of diffractive, distributed telescopy and precision satellite formation flying.</p>
<p><n>
</n></p>
<p><img alt="" src="satellite.png" style="width:300px" /></p>
<h2 id="paper-outline">Paper Outline</h2>
<p>Chapter 2 introduces classes of image registration and describes popular registration methods from each class.
Chapter 3 introduces the idea of subpixel registration, its uses, and gives a summary of a fast subpixel registration algorithm which is used to derive a new fast multi-frame subpixel algorithm, described in chapter 4.
Chapter 5 contains numerical registration experiments under various settings, a description of the pipeline used to generate the test images, and some tests involving other classes of images unrelated to the VISORS project.</p>
<h1 id="review-of-registration-methods">Review of Registration Methods</h1>
<h1 id="subpixel-registration">Subpixel Registration</h1>
<h1 id="multiframe-subpixel-registration">Multiframe Subpixel Registration</h1>
<h1 id="numerical-experiments">Numerical Experiments</h1>
<h1 id="conclusion">Conclusion</h1>
<!-- ### Optimization-Based Methods -->

<!-- Many of the above methods introduce various similarity metrics as measures of alignment between registration candidates.  They mostly rely on an exhaustive search through all potential registration candidates and as such are restricted to situations where the set of all possible $f$ candidates are small, usually simple translation.  In situations where the number of parameters controlling $f$ is large, such as elastic transform, it is appropriate to make use optimization methods that can find minima or maxima in the chosen similarity measure in a feasible amount of time. -->

<!-- # Summary -->

<!-- Area-based methods are preferred when images have salient details and information is provided by pixel intensities rather than shapes and structures in the image.  The images' intensities must be similar or at least statistically related.  The set of candidate transforms $f$ that can be searched is generally limited to translation with small amounts of rotation or skew, but there are some extensions to methods that support large rotations or skews so long as the degrees of freedom of $f$ remains small.  Pyramid search techniques and sophisticated optimization strategies are available to more quickly find extrema of the similarity measure. -->

<!-- Feature-based methods are generally utilized when shapes in structures in the image pair contain more alignment information than the pixel intensities, such as between a picture of an object and a computer model of an object.  The drawback of these methods is that such features can be hard to detect and match with each other.  It is critical that chosen feature detector be robust against any differences between the images. -->

<!-- | Method | Application | Noise | -->

<!-- |--------|-------------|-------| -->

<!-- | foo    |             |       | -->

<!-- |        |             |       | -->

<h1 id="references">References</h1>
<ul>
<li><a href="https://www.google.com/search?q=survey%20of%20mutual%20information%20based%20registration">A Survey of Mutual Information Based Registration</a> - Pluim, Maintz, Viergever 2003</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Motion Estimation - Konrad&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:zitova">
<p><a href="https://www.sciencedirect.com/science/article/pii/S0262885603001379/pdfft?md5=9ac6884a88ac624d4861de8fe7666e27&amp;pid=1-s2.0-S0262885603001379-main.pdf">Image registration methods: a survey</a> - Zitova, Flusser 2003&#160;<a class="footnote-backref" href="#fnref:zitova" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:brown">
<p>Brown Survey Paper&#160;<a class="footnote-backref" href="#fnref:brown" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:brown" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:oktem">
<p>F. S. Oktem, F. Kamalabadi, and J. M. Davila, "High-resolution computational spectral imaging with photon sieves," in 2014 IEEE International Conference on Image Processing (ICIP). IEEE, 2014, pp. 5122-5126.&#160;<a class="footnote-backref" href="#fnref:oktem" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:farsiu2004">
<p>Fast and Robust Multiframe Super Resolution - 2004, 2343 Citations&#160;<a class="footnote-backref" href="#fnref:farsiu2004" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5008923">Barnea, Silverman 1972</a>&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:5">
<p><a href="https://ieeexplore.ieee.org/document/4767966">De Castro, Morandi 1987</a>&#160;<a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:6">
<p><a href="https://www.osapublishing.org/ol/viewmedia.cfm?uri=ol-33-2-156&amp;seq=0">Guizar-Sicairos, Thurman, Fienup 2008</a>&#160;<a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn:7">
<p><a href="https://ieeexplore.ieee.org/document/988953">Foroosh, Zerubia, Berthod 2002</a>&#160;<a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="fn:8">
<p><a href="http://www.jprr.org/index.php/jprr/article/view/355">Sarvaiya, Patnaik, Kothari 2012</a>&#160;<a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
<li id="fn:9">
<p>Feature-Based Deformable Image Registration with RANSAC Based Search Correspondence - Colleu, Shen, Matuszewski, Shark, Cariou&#160;<a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 11 in the text">&#8617;</a></p>
</li>
<li id="fn:10">
<p>An Automatic Satellite Image Registration Technique Based on Harris Corner Detection and Random Sample Consensus (RANSAC) Outlier Rejection Model - Misra, Moorthi, Dhar, Ramakrishnan&#160;<a class="footnote-backref" href="#fnref:10" title="Jump back to footnote 12 in the text">&#8617;</a></p>
</li>
</ol>
</div>
  </article>
</main>
<footer>
    <p>All source code and data available <a href="https://github.com/UIUC-SINE/uiuc-sine.github.io">here</a></p>
    <p>SINE UIUC</p>
</footer>
</body>
</html>